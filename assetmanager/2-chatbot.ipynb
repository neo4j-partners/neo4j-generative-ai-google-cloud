{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62d8979e",
   "metadata": {},
   "source": [
    "# Chatbot\n",
    "In this notebook, we'll build a chatbot.  The chatbot interface will use gradio.  Underlying the chatbot is the Neo4j knowledge graph we built in previous labs.  The chatbot uses generative AI and langchain.  We take a natural language question, convert it to Neo4j Cypher using generative AI and run that against the database.  We take the response from the database and use generative AI again to convert that back to natural language before presenting it to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c76c4f",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acec75a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --user \"langchain==0.0.314\"\n",
    "%pip install --user gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea81aba-c3e5-435b-9a9a-86d0a65da844",
   "metadata": {},
   "source": [
    "## Base Example Without Grounding\n",
    "Before grounding with the Neo4j, let's setup up a baseline that just uses an LLM to answer questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db1b0b6-57c6-427f-b007-1419d1213dd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import VertexAI\n",
    "\n",
    "base_chain = VertexAI(model_name='text-bison', max_output_tokens=2048, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e6d342-e9dc-4533-955e-601e181011ae",
   "metadata": {},
   "source": [
    "We can now ask a simple finance question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f933a2d9-53fd-41df-8b5a-248885b0dbda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_response = base_chain(\"\"\"What are the top 10 investments for Blackrock?\"\"\")\n",
    "print(f\"Final answer: {base_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f64a21-8347-4eec-a83a-8044f824a3d8",
   "metadata": {},
   "source": [
    "While this answer looks reasonable, we have no real way to know how the LLM came it with it, or where it was sourced from.\n",
    "\n",
    "Here is a more complicated example where we expect the LLM to understand some more specific terminology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae43c396-39d5-4511-bc3d-a56a1880afc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_response = base_chain(\"\"\"Which managers own FAANG stocks?\"\"\")\n",
    "print(f\"Final answer: {base_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533cf10e-45f4-427d-bda6-a7ae4c706d92",
   "metadata": {},
   "source": [
    "In this case, it looks like the LLM understands the ubiquitous acronym FAANG but, unsurprisingly, the results indicate it doesn't understand manager within the context of our data model.  In your use case, you may have lots of specific terminology/ontology like this that you would need a chatbot to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a087cb5-f99d-4c7c-bdac-14ec8cd411cb",
   "metadata": {},
   "source": [
    "## Grounding LLMs with Neo4j\n",
    "Now let's create a chatbot that is grounded with Neo4j. Below is the pattern we will follow with LangChain:\n",
    "\n",
    "![](images/langchain.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c044359a",
   "metadata": {},
   "source": [
    "## Cypher Generation\n",
    "We have to use a prompt template that: \n",
    "1. Clearly states what schema to use \n",
    "2. Provides principles the chatbot should follow in generating responses\n",
    "3. Demonstrates few-shot examples to help the chatbot be more accurate in its query generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e5c4a7-9821-4fe8-9b26-49dc96d89103",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CYPHER_GENERATION_TEMPLATE = \"\"\"You are an expert Neo4j Cypher translator who understands the question in english and convert to Cypher strictly based on the Neo4j Schema provided and following the instructions below:\n",
    "1. Generate Cypher query compatible ONLY for Neo4j Version 5\n",
    "2. Do not use EXISTS, SIZE keywords in the cypher. Use alias when using the WITH keyword\n",
    "3. Please do not use same variable names for different nodes and relationships in the query.\n",
    "4. Use only Nodes and relationships mentioned in the schema\n",
    "5. Always enclose the Cypher output inside 3 backticks\n",
    "6. Always do a case-insensitive and fuzzy search for any properties related search. Eg: to search for a Company name use `toLower(c.name) contains 'neo4j'`\n",
    "7. Candidate node is synonymous to Manager\n",
    "8. Always use aliases to refer the node in the query\n",
    "9. 'Answer' is NOT a Cypher keyword. Answer should never be used in a query.\n",
    "10. Please generate only one Cypher query per question. \n",
    "11. Cypher is NOT SQL. So, do not mix and match the syntaxes.\n",
    "12. Every Cypher query always starts with a MATCH keyword.\n",
    "\n",
    "Schema:\n",
    "{schema}\n",
    "Samples:\n",
    "Question: Which fund manager owns most shares? What is the total portfolio value?\n",
    "Answer: MATCH (m:Manager) -[o:OWNS]-> (c:Company) RETURN m.managerName as manager, sum(distinct o.shares) as ownedShares, sum(o.value) as portfolioValue ORDER BY ownedShares DESC LIMIT 10\n",
    "\n",
    "Question: Which fund manager owns most companies? How many shares?\n",
    "Answer: MATCH (m:Manager) -[o:OWNS]-> (c:Company) RETURN m.managerName as manager, count(distinct c) as ownedCompanies, sum(distinct o.shares) as ownedShares ORDER BY ownedCompanies DESC LIMIT 10\n",
    "\n",
    "Question: What are the top 10 investments for Vanguard?\n",
    "Answer: MATCH (m:Manager) -[o:OWNS]-> (c:Company) WHERE toLower(m.managerName) contains \"vanguard\" RETURN c.companyName as Investment, sum(DISTINCT o.shares) as totalShares, sum(DISTINCT o.value) as investmentValue order by investmentValue desc limit 10\n",
    "\n",
    "Question: What other fund managers are investing in same companies as Vanguard?\n",
    "Answer: MATCH (m1:Manager) -[:OWNS]-> (c1:Company) <-[o:OWNS]- (m2:Manager) WHERE toLower(m1.managerName) contains \"vanguard\" AND elementId(m1) <> elementId(m2) RETURN m2.managerName as manager, sum(DISTINCT o.shares) as investedShares, sum(DISTINCT o.value) as investmentValue ORDER BY investmentValue LIMIT 10\n",
    "\n",
    "Question: What are the top investors for Apple?\n",
    "Answer: MATCH (m1:Manager) -[o:OWNS]-> (c1:Company) WHERE toLower(c1.companyName) contains \"apple\" RETURN distinct m1.managerName as manager, sum(o.value) as totalInvested ORDER BY totalInvested DESC LIMIT 10\n",
    "\n",
    "Question: What are the other top investments for fund managers investing in Apple?\n",
    "Answer: MATCH (c1:Company) <-[:OWNS]- (m1:Manager) -[o:OWNS]-> (c2:Company) WHERE toLower(c1.companyName) contains \"apple\" AND elementId(c1) <> elementId(c2) RETURN DISTINCT c2.companyName as company, sum(o.value) as totalInvested, sum(o.shares) as totalShares ORDER BY totalInvested DESC LIMIT 10\n",
    "\n",
    "Question: What are the top investors in the last 3 months?\n",
    "Answer: MATCH (m:Manager) -[o:OWNS]-> (c:Company) WHERE date() > o.reportCalendarOrQuarter > o.reportCalendarOrQuarter - duration({{months:3}}) RETURN distinct m.managerName as manager, sum(o.value) as totalInvested, sum(o.shares) as totalShares ORDER BY totalInvested DESC LIMIT 10\n",
    "\n",
    "Question: What are top investments in last 6 months for Vanguard?\n",
    "Answer: MATCH (m:Manager) -[o:OWNS]-> (c:Company) WHERE toLower(m.managerName) contains \"vanguard\" AND date() > o.reportCalendarOrQuarter > date() - duration({{months:6}}) RETURN distinct c.companyName as company, sum(o.value) as totalInvested, sum(o.shares) as totalShares ORDER BY totalInvested DESC LIMIT 10\n",
    "\n",
    "Question: Who are Apple's top investors in last 3 months?\n",
    "Answer: MATCH (m:Manager) -[o:OWNS]-> (c:Company) WHERE toLower(c.companyName) contains \"apple\" AND date() > o.reportCalendarOrQuarter > date() - duration({{months:3}}) RETURN distinct m.managerName as investor, sum(o.value) as totalInvested, sum(o.shares) as totalShares ORDER BY totalInvested DESC LIMIT 10\n",
    "\n",
    "Question: Which fund manager under 200 million has similar investment strategy as Vanguard?\n",
    "Answer: MATCH (m1:Manager) -[o1:OWNS]-> (:Company) <-[o2:OWNS]- (m2:Manager) WHERE toLower(m1.managerName) CONTAINS \"vanguard\" AND elementId(m1) <> elementId(m2) WITH distinct m2 AS m2, sum(distinct o2.value) AS totalVal WHERE totalVal < 200000000 RETURN m2.managerName AS manager, totalVal*0.000001 AS totalVal ORDER BY totalVal DESC LIMIT 10\n",
    "\n",
    "Question: Who are common investors in Apple and Amazon?\n",
    "Answer: MATCH (c1:Company) <-[:OWNS]- (m:Manager) -[:OWNS]-> (c2:Company) WHERE toLower(c1.companyName) contains \"apple\" AND toLower(c2.companyName) CONTAINS \"amazon\" RETURN DISTINCT m.managerName LIMIT 50\n",
    "\n",
    "Question: What are Vanguard's top investments by shares for 2023?\n",
    "Answer: MATCH (m:Manager) -[o:OWNS]-> (c:Company) WHERE toLower(m.managerName) CONTAINS \"vanguard\" AND date({{year:2023}}) = date.truncate('year',o.reportCalendarOrQuarter) RETURN c.companyName AS investment, sum(o.value) AS totalValue ORDER BY totalValue DESC LIMIT 10\n",
    "\n",
    "Question: What are Vanguard's top investments by value for 2023?\n",
    "Answer: MATCH (m:Manager) -[o:OWNS]-> (c:Company) WHERE toLower(m.managerName) CONTAINS \"vanguard\" AND date({{year:2023}}) = date.truncate('year',o.reportCalendarOrQuarter) RETURN c.companyName AS investment, sum(o.shares) AS totalShares ORDER BY totalShares DESC LIMIT 10\n",
    "\n",
    "Question: {question}\n",
    "Answer: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09e46b3",
   "metadata": {},
   "source": [
    "Now let’s create a LangChain prompt template.  \n",
    "\n",
    "This template defines the parameter inputs for the prompt sent to the Cypher generation bot.  In our example, the inputs will be schema and question.  The question comes from the end user.  The LangChain GraphCypherQAChain automatically inserts the schema via a built-in method to Neo4jGraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86addda3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "CYPHER_GENERATION_PROMPT = PromptTemplate(\n",
    "    input_variables=['schema','question'], validate_template=True, template=CYPHER_GENERATION_TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f63c46",
   "metadata": {},
   "source": [
    "Provide your Neo4j credentials.  We need the DB conection URL and password.  The username is neo4j by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc7bc10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# username is neo4j by default\n",
    "NEO4J_USERNAME = 'neo4j'\n",
    "\n",
    "# You will need to change these to match your credentials\n",
    "NEO4J_URI = 'neo4j+s://89d38941.databases.neo4j.io'\n",
    "NEO4J_PASSWORD = '65ye5i0QZfje9J6KU7CPLPXSghRhh3kXrNTJKqHaXd'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180e2e7d",
   "metadata": {},
   "source": [
    "We need to connect to the graph via LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbc48f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.graphs import Neo4jGraph\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=NEO4J_URI, \n",
    "    username=NEO4J_USERNAME, \n",
    "    password=NEO4J_PASSWORD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1263e2",
   "metadata": {},
   "source": [
    "We define our `chain` object (specifically a`GraphCypherQAChain`) using Anthropic Claude V2 LLM.\n",
    "\n",
    "`GraphCypherQAChain` also takes a ‘Neo4jGraph’ so it can handle the chatbot process end-to-end, from taking the user question and translating to Cypher to executing the query, getting results, translating back to natural language, and returning to the user. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f94866b-7729-46a5-85cb-c11fb364eefa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import GraphCypherQAChain\n",
    "import json\n",
    "\n",
    "chain = GraphCypherQAChain.from_llm(\n",
    "    VertexAI(model_name='code-bison@002', max_output_tokens=2048, temperature=0.0),\n",
    "    graph=graph,\n",
    "    cypher_prompt=CYPHER_GENERATION_PROMPT,\n",
    "    verbose=True,\n",
    "    return_direct=True\n",
    ")\n",
    "\n",
    "def chat(que):\n",
    "    r = chain(que)\n",
    "    print(r)\n",
    "    llm=VertexAI(model_name='text-bison', max_output_tokens=2048, temperature=0.0)\n",
    "    summary_prompt_tpl = f\"\"\"Human: \n",
    "    Fact: {json.dumps(r['result'])}\n",
    "\n",
    "    * Summarise the above fact as if you are answering this question \"{r['query']}\"\n",
    "    * When the fact is not empty, assume the question is valid and the answer is true\n",
    "    * Do not return helpful or extra text or apologies\n",
    "    * Just return summary to the user. DO NOT start with Here is a summary\n",
    "    * List the results in rich text format if there are more than one results\n",
    "    Assistant:\n",
    "    \"\"\"\n",
    "    return llm(summary_prompt_tpl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d1590f",
   "metadata": {},
   "source": [
    "Below we have a few examples of how we can get answers from the chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed32736-43f8-4c9c-b90e-69932461bbd3",
   "metadata": {},
   "source": [
    "## Why Ground Your LLM?\n",
    "Recall our base example where we asked for the top 10 Rempart investments?  We got an answer that looked like it may be reasonable, but we couldn't validate it or track sources.  We also asked what managers own FAANG stocks, and for that, we unsurprisingly received the wrong answers for our use case.\n",
    "\n",
    "Let's try again grounding with Neo4j. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9e6bf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "r2 = chat(\"\"\"What are the top 10 investments for Brookfield?\"\"\")\n",
    "print(f\"Final answer: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65434f3-f818-46ca-9fc8-0daa4a77942b",
   "metadata": {},
   "source": [
    "Notice that this answer is different from our base example, and this time we have the Cypher logic used to obtain the answer from our database. This means that we can trace back how we came up with this answer and make any adjustments to our database or prompt if we need to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7da574-8c23-4c78-b35f-bb4e86cdb291",
   "metadata": {},
   "source": [
    "Now lets try the FAANG question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806d1464-7202-4d66-af52-3a6ffe8668d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "r3 = chat(\"\"\"Which managers own FAANG stocks?\"\"\")\n",
    "print(f\"Final answer: {r3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0686244-bb6b-4231-aebf-70b3c777ef25",
   "metadata": {},
   "source": [
    "Here again, we notice the traceability with Cypher, and because we engineered our prompt to include our schema, it understood what “manager” means in the context of our use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d119d1-5b84-4665-985c-42eeeca49779",
   "metadata": {},
   "source": [
    "## Why Ground your LLM with Neo4j?\n",
    "There are 3 primary reasons to ground your LLM with Neo4j specifically:\n",
    "1. __Grounding for more complex question handling__: Multi-hop knowledge retrieval across connected data. Connections between data points are calculated before query time. \n",
    "2. __Enterprise reliability and security__: Fine-grained security so the chatbot only accesses information the user has permission to. Autonomous clustering for horizontal scaling.  Fully managed service in the cloud through Aura. \n",
    "3. __Performance__: fast queries with high concurrency for many users.\n",
    "\n",
    "We can explore point 1 with more complex questions below.\n",
    "\n",
    "A question requiring ~4 hops (would be joins in the relational world).  Having a knowledge graph with relationships calculated before query time allows us to answer the question quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a980deb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "r4 = chat(\"\"\"What are other top investments for fund managers investing in Lowes?\"\"\")\n",
    "print(f\"Final answer: {r4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbca6ab-01d6-4697-a58f-7978567ff66b",
   "metadata": {},
   "source": [
    "and more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588955da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "r5 = chat(\"\"\"Please get me common investors between Tesla and Costco\"\"\")\n",
    "print(f\"Final answer: {r5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8b038e",
   "metadata": {},
   "source": [
    "## Grounded Chatbot\n",
    "Now we will use Gradio to deploy a chat interface with our chain behind it.\n",
    "\n",
    "The below code deploys a Gradio application.  You can access the app via a local URL. A publicly sharable URL is also provided (sharable for 3 days)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbe1766",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import typing_extensions\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key = \"chat_history\", return_messages = True)\n",
    "\n",
    "def chat_response(input_text,history):\n",
    "\n",
    "    try:\n",
    "        return chat(input_text)\n",
    "    except:\n",
    "        # a bit of protection against exposed error messages\n",
    "        # we could log these situations in the backend to revisit later in development\n",
    "        return \"I'm sorry, there was an error retrieving the information you requested.\"\n",
    "\n",
    "interface = gr.ChatInterface(fn = chat_response,\n",
    "                             title = \"Investment Chatbot\",\n",
    "                             description = \"powered by Neo4j\",\n",
    "                             theme = \"soft\",\n",
    "                             chatbot = gr.Chatbot(height=500),\n",
    "                             undo_btn = None,\n",
    "                             clear_btn = \"\\U0001F5D1 Clear chat\",\n",
    "                             examples = [\"What are the top 10 investments for Rempart?\",\n",
    "                                         \"Which manager owns FAANG stocks?\",\n",
    "                                         \"What are other top investments for fund managers investing in Exxon?\",\n",
    "                                         \"What are Rempart's top investments by value for 2023?\",\n",
    "                                         \"Who are the common investors between Tesla and Costco?\",\n",
    "                                         \"Who are Tesla's top investors in last 3 months?\"])\n",
    "\n",
    "interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5197afd",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we went through the steps of connecting a LangChain agent to a Neo4j database and using it to generate Cypher queries in response to user requests via LLMs on VertexAI, thus grounding the LLM with a knowledge graph.\n",
    "\n",
    "While we used the `code-bison` model here, this approach can be generalized to other VertexAI LLMs.  This process can also be augmented with additional steps around the generation chain to customize the experience for specific use cases.  \n",
    "\n",
    "The critical takeaway is the importance of Neo4j Knowledge Graph as a grounding database to: \n",
    "* Anchor your chatbot to reality as it generates responses and \n",
    "* Enable your LLM to provide answers enriched with relevant enterprise data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a91ad1-df6a-4c73-ab56-693c737d7f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m111"
  },
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
